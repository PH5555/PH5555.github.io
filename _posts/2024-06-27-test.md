---
layout: post
title: NLP
tags: [인공지능]
comments: true
---

## 방법론

텍스트 데이터에서 주제를 찾는 방법에는 크게 `Topic Modeling` 과 ` Text Clustering`이 있다.

## 전처리

기계학습에 있어서 데이터 전처리는 필수적이다. NLP를 위한 텍스트 데이터에는 다음과 같은 전처리를 진행할 수 있다.

1. URL, 이메일, 전화번호, 구두점 제거
2. 태그, 이모티콘, 기호 제거
3. 불용어 제거
4. 중복 제거
5. 맞춤법 제거

```
원본 리뷰: {'지루한 영화와 에피소드가 모두 포함되어 있습니다'} 
전처리된 리뷰: {'지루한 영화 에피소드'}
```

## 원 핫 인코딩

기계는 문자보다는 숫자를 더 잘 처리한다. 이를 위해서 문자를 숫자로 바꾸는 여러 방법이 있는데 `원 핫 인코딩`은 그 방법 중 제일 기본적인 방법이다.

> 단어 집합
> 서로 다른 단어들의 집합. 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓으면 단어집합이라고 한다. 텍스트에 단어가 5000개면 단어집합도 5000개이고, 각 단어에 인덱스가 부여된다.

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다.

"나는 자연어 처리를 배운다" 라는 문장을 예시로 들어보겠다.

`Okt 형태소 분석기`를 통해서 문장을 단어 단위로 나눈다. 결과로 ['나', '는', '자연어', '처리', '를', '배운다'] 배열이 나오게 된다.

그리고 해당 배열의 각 요소에 인덱스를 부여한다. 그리고 '자연어' 라는 단어의 `원 핫 벡터'를 추출해보면 [0, 0, 1, 0, 0, 0] 이 나오게 된다.

#### 한계점

먼저 단어의 갯수가 늘어날 수록 벡터도 늘어나서 공간이 낭비된다는 단점이 있다. 그리고 단어간의 유사도를 판별할 수 없다. 가령 `아산 숙소`라고 검색하면 `아산 호텔` `아산 게스트하우스` 와 같은 것도 같이 검색되면 좋지만 
유사도를 판별할 수 없어 불가능하다. 단어의 의미를 고려하여 다차원 벡터를 만든것이 LSA, worde2vec 등이 있다.

## Word2Vec

원 핫 인코딩은 단어벡터 간 유사도를 비교할 수 없었다. 유사도를 반영하는 가장 대표적인 방법으로 Word2Vec이 있다. 

Word2Vec은 원 핫 벡터처럼 벡터의 차원이 단어 집합의 크기와 같을 필요가 없다. 예를들어 강아지란 단어를 표현하기 위해 사용자가 벡터의 차원을 설정할 수 있고, 각 차원의 값은 실수값을 가진다. Word2Vec의 학습방법에는 
`CBOW`와 `Skip-Gram` 두가지가 있다.

1. CBOW

주변에 있는 단어들을 보고 중간 단어를 예측하는 방법이다.

"The fat cat sat on the mat" 을 예문으로 두고 설명해 보겠다.

윈도우 크기를 2라고 하고 cat을 예측한다고 하면 앞 뒤 단어 2개씩이 입력값이 된다. 즉 윈도우 크기가 2라고 하면 실제로 입력값으로 들어가는 수는 2N이다. 

![input](/assets/img/word2vec_input.PNG)

![input1](/assets/img/word2vec_stru.PNG)

CBOW의 인공신경망을 간단히 도식하면 다음과 같다. 입력층에 예측 단어의 앞뒤 단어가 원 핫 벡터로 들어간다. 그리고 은닉층이 다수인 딥러닝과 다르게 은닉층이 하나만 들어가는 얕은 신경망이다. 

![input2](/assets/img/word2vec_1.PNG)

입력층과 투사층 사이의 가중치 W는 V x M 행렬이고 투사층과 출력층 사이의 가중치는 M x V 행렬이다. 이 때 두 행렬은 역 관계의 행렬은 아니다. 입력층의 원 핫 벡터와 가중치 W 와 연산을 하면 가중치 W의 i 번째 인덱스가 나오게 된다. 
이 가중치를 잘 학습 시키는 것이 목적이다. 투사층에서 가중치를 곱한 입력층이 오면 이 값들의 평균을 구한다. 평균이 다음 가중치와 곱해지고 소프트맥스 함수를 지나게 되는데 소프트맥스 함수를 지나면 각 원소의 값은 0과 1 사이가 되고, 
합은 1이 된다. 마지막 스코어 벡터의 값은 중심 단어일 확률을 나타낸다. 

![input3](/assets/img/word2vec_2.PNG)

역전파를 하여 가중치의 값을 학습시킨다.

2. Skip-Gram

Skip-Gram 은 중간 단어를 보고 주변 단어를 예측하는 것이다. 입력값이 하나로 들어오기 때문에 CBOW와 다르게 투사층에서 평균을 구하지 않는다. 
